---
layout: post
title: low rank deep learning
modified:
categories: ['Machine Learning']
excerpt:
tags: []
image:
  feature:
date: 2016-01-04T10:46:55+08:00
---


近些年深度神经网络发展迅速，深度越来越深，从几层到十几再到一百多，相信随着计算资源增多，会有更为惊人的设计。但有一个问题不得不重视，就是深度神经网络中存在大量的冗余参数，也就是过参数化，产生的问题主要是：

1.  学习缺乏理解，也就是通过大量参数，可能过拟合了结果，缺乏泛化性能；
2.  对于一些天生数据较少的问题无法解决，例如医学数据，存在大量负样本，正样本采集困难；
3.  计算性能较差，带有全连接层的网络，通常训练大量集中在全连接层，更深的VGG等网络训练也很慢。

现在有一种思维是，数据足够多，算力足够大，总能得到好结果，但在很多问题上不可行。因此如果可以有效减少参数数量，利用深度学习的结构，或许可以得到一些收获，例如：

1.  以较小的准确率损失，大量的提升速度，同时减小计算量，和计算功耗，使得一些应用场景中引入深度学习成为可能；
2.  对于数据较少的问题，一定程度上提高神经网络的可用性。

最近倒是有一些文章开始关注除了weighted-decay方法外的正则参数，来调整神经网络训练。但添加$\ell_2$正则的算法并不能降低有效自由度，需要考虑其他正则。后来Bengio做了一个sparse convolution neural network的文章，但没了下文，之后再没见过效果很好的类似工作。这篇文章的一个基本假设是学习过程中大量的kernel是稀疏的，但这个假设也许不成立。

之后有一些工作开始试图从低秩的角度去简化神经网络。比较好的两个工作时Yann LeCun组的Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation和牛津大学的Max Jaderberg的Speeding Up Convolutional Neural Networks with Low Rank Expansions.两篇文章都是2014年的工作，牛津工作preprint靠前一点，下面就分别讲一下这两篇文章。

牛津Jaderberg工作的核心思想是只需要一小部分的修改就可以使用的减小参数数量的模型，最终在caffe中实现。对于一个输入$x\in R^{H\times W}$的图像，经过卷积核$F={f_i},i\in [1,2,...,N]$输出的feature map为$Y={y_1,y_2,...,y_N}$, $y_i\in R^{H'\times W'}$。若卷积核为一个$d\times d$的二维卷积，则一个channel的参数空间数量为$O(d^2NH'W')$。一种合理的逼近方法是使用M个filter $S={s_1,...,s_M}$来生成每个filter$y_i=\sum_{k=1}^Ma_{ik}s_k*x+\epsilon_i$，其参数空间复杂度为$O((Md^2+MN)H'W')$。若$M\leq\frac{d^2N}{d^2+N}$，就可以得到算法性能的提升。


![Alt text](/Users/yancz1989/Documents/working/doc/summary/images/ml_low_rank.png "图1")

如上图所示，是卷积低秩逼近的方式。若Channel数量为1，则可以用(a)中所示的方法将卷积核看作是由N各秩为1的矩阵组成的。使用低秩逼近，第一种方案是使用M个filter$$S={s_m^c,m\in[1,...,M]}$$作为基，对于每一个通道，将$$W_n^c\simeq\sum_{m=1}^Ma_n^{am}*z_m^c$$作为一个逼近。如图(b)所示，由M个基经过线性组合组成N个基，所需的计算总计$$O(MC(d+N)H'W')$$。另一种是如(c)所示，首先选择K个$$d\times 1\times 1$$的基向量，由输入z与其进行卷积计算，得到的2D输出在与N个$$d\times 1\times K$$的卷积核进行卷积得出N个filter，参数空间复杂度为$$O(K(N+C)dW'H')$$。这两种方法都是以线性组合的方式逼近一个满秩的卷积核。在得出低秩逼近方法后，需要解决优化问题，也有两种策略，即核范数正则和矩阵分解法。优化形式为

$$\min_{\{s_m\},\{a_n\}}\sum_{n=1}^N\sum_{c=1}^C{||W_n^c-\sum_{m=1}^Ma_n^{cm}s_m||^2_2+\lambda\sum_{m=1}^M||s_m||_*}$$

$$\min_{\{s_m\},\{a_n\}}\sum_{n=1}^N\sum_{c=1}^C{||W_n^c-\sum_{k=1}^K{h_n^k*v_k^c}||_2^2}$$

最终的出的结果，在ICDAR2003数据集上，使用方法2进行低秩表示，准确率没有损失，且得到了2.5的加速，而使用方法1，准确率下降了1个百分点，而得到了4.5倍的速度提升。 Yann LeCun组的工作的思路与牛津组思路类似，起因也是由参数冗余（引用文献的结论是只需要5%的参数就可以完成任务），做法是通过对底层的卷积核使用低秩逼近，在对上层进行调参，直到性能恢复，使用的数据集是ImageNet。他提出了一个不同的思路是，对一个四维张量进行逼近，直接使用F范数是不合理的，需要进行空间转换，因此带入了马氏距离。但码是距离参数更大，特别是在深度神经网络这种堪称超高维的情形，因此最终徐娜泽使用一个线性组合作为马氏距离的替代。高维的张量，如3维张量可以表示为最小化

$$||W-\alpha\otimes\beta\otimes\gamma||_F,\alpha\in R^m,\beta\in R^n,\gamma\in R^k$$

，优化使用梯度优化，最终得到了在state of the art上得到了2.5倍加速（应该是牛津组的基础上）。

显然减小参数数量对于计算的影响是很大的，但这里似乎需要探讨的问题也有很多：

1.  低秩张量SVD分解并不唯一，从不同的维度开始得到的结果也不同，对于提升运算速度和神经网络性能是否有影响？
2.  牛津组对于不同filter都假设低秩，实质是寻找一组基对各filter都进行逼近，但是否存在一些结构化的地址，例如通道地址，也就是对于RGB图像，同一个filter不同通道所组成的张量是否低秩？
3.  低秩分量的理解。已有一些工作试图对深度神经网络学习出的卷积进行可视化和解释，那么各低秩分量在学习中是否有一定物理意义？这个物理意义与我们所希望得到的概念是否有一些对应关系？
